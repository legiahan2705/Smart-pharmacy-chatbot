import os
import json
import gc
import requests
import base64
import io         
import edge_tts   # <--- S·ª≠a l·ªói v√†ng cho TTS
from dotenv import load_dotenv
from typing import TypedDict, Literal

# --- FastAPI Imports ---
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel 
from fastapi import FastAPI, UploadFile, File 
from fastapi.responses import StreamingResponse 
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# --- LangChain Imports ---
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings, HarmBlockThreshold, HarmCategory
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.memory import MemorySaver

import pandas as pd
import re

# =======================================================
# 0. KH·ªûI T·∫†O & C·∫§U H√åNH
# =======================================================
load_dotenv()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =======================================================
# 1. X·ª¨ L√ù D·ªÆ LI·ªÜU (D√ôNG PARQUET ƒê·ªÇ TƒÇNG T·ªêC KH·ªûI ƒê·ªòNG)
# =======================================================
def load_and_clean_data() -> pd.DataFrame:
    parquet_path = "data/optimized_db.parquet"
    source_json_path = "data/longchau_selected.json"

    # 1. Load Cache (∆Øu ti√™n)
    if os.path.exists(parquet_path):
        print("‚ö° [Pandas] T√¨m th·∫•y Cache Parquet. ƒêang t·∫£i...")
        try:
            return pd.read_parquet(parquet_path)
        except Exception as e:
            print(f"‚ö†Ô∏è Cache l·ªói ({e}), s·∫Ω x·ª≠ l√Ω l·∫°i t·ª´ ƒë·∫ßu...")

    # 2. X·ª≠ l√Ω l·∫ßn ƒë·∫ßu (T·ªëi ∆∞u b·ªô nh·ªõ)
    print("üê¢ [Pandas] B·∫Øt ƒë·∫ßu ƒë·ªçc file JSON...")
    try:
        # B∆∞·ªõc A: ƒê·ªçc file
        with open(source_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"   -> ƒê√£ ƒë·ªçc xong {len(data)} s·∫£n ph·∫©m. ƒêang chu·∫©n h√≥a d·ªØ li·ªáu...")

        # B∆∞·ªõc B: X·ª≠ l√Ω tr·ª±c ti·∫øp tr√™n bi·∫øn 'data' (Kh√¥ng t·∫°o b·∫£n sao normalized_data)
        for item in data:
            # L·∫•y danh s√°ch key ƒë·ªÉ tr√°nh l·ªói runtime khi dictionary thay ƒë·ªïi size
            keys = list(item.keys()) 
            for key in keys:
                if key.startswith("Th√†nh ph·∫ßn"):
                    item["Th√†nh ph·∫ßn"] = item.pop(key) # ƒê·ªïi t√™n key c≈© th√†nh m·ªõi v√† x√≥a key c≈© ngay
                elif key.startswith("C√¥ng d·ª•ng"):
                    item["C√¥ng d·ª•ng"] = item.pop(key)

        print("   -> ƒêang chuy·ªÉn sang DataFrame...")
        
        # B∆∞·ªõc C: T·∫°o DataFrame v√† X√ìA NGAY bi·∫øn data ƒë·ªÉ gi·∫£i ph√≥ng RAM
        df = pd.DataFrame(data)
        del data # X√≥a bi·∫øn data
        gc.collect() # √âp bu·ªôc d·ªçn d·∫πp b·ªô nh·ªõ ngay l·∫≠p t·ª©c
        
        print("   -> ƒêang l√†m s·∫°ch c·ªôt gi√° v√† ƒëi·ªÅn d·ªØ li·ªáu thi·∫øu...")
        
        # B∆∞·ªõc D: X·ª≠ l√Ω c·ªôt gi√° (D√πng vectorized operation nhanh h∆°n apply)
        # Chuy·ªÉn v·ªÅ string tr∆∞·ªõc ƒë·ªÉ tr√°nh l·ªói
        df['Gi√° b√°n'] = df['Gi√° b√°n'].astype(str)
        # D√πng Regex tr√≠ch xu·∫•t s·ªë tr·ª±c ti·∫øp (nhanh h∆°n loop)
        df['price_int'] = df['Gi√° b√°n'].str.replace(r'[^\d]', '', regex=True)
        df['price_int'] = pd.to_numeric(df['price_int'], errors='coerce').fillna(0).astype(int)

        # B∆∞·ªõc E: ƒêi·ªÅn d·ªØ li·ªáu tr·ªëng
        cols_to_fill = ['Nh√† s·∫£n xu·∫•t', 'N∆∞·ªõc s·∫£n xu·∫•t', 'Xu·∫•t x·ª© th∆∞∆°ng hi·ªáu', 'Danh m·ª•c', 'D·∫°ng b√†o ch·∫ø', 'Quy c√°ch', 'Th√†nh ph·∫ßn', 'L∆∞u √Ω', 'B·∫£o qu·∫£n', 'C√¥ng d·ª•ng', 'ƒê∆°n v·ªã']
        
        # Ch·ªâ ƒëi·ªÅn nh·ªØng c·ªôt th·ª±c s·ª± t·ªìn t·∫°i trong df
        existing_cols = [c for c in cols_to_fill if c in df.columns]
        df[existing_cols] = df[existing_cols].fillna('')
        
        # B∆∞·ªõc F: Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu ƒë·ªÉ l∆∞u Parquet an to√†n
        print("   -> ƒêang l∆∞u Cache Parquet (B∆∞·ªõc cu·ªëi)...")
        
        # Chuy·ªÉn t·∫•t c·∫£ v·ªÅ string (tr·ª´ price_int) ƒë·ªÉ tr√°nh l·ªói format c·ªßa Parquet
        for col in df.columns:
            if col != 'price_int':
                df[col] = df[col].astype(str)
        
        # L∆∞u file
        df.to_parquet(parquet_path, index=False)
        print(f"‚úÖ [Pandas] X·ª≠ l√Ω xong v√† ƒë√£ l∆∞u Cache v√†o {parquet_path}.")
        
        return df

    except Exception as e:
        print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG KHI X·ª¨ L√ù DATA: {e}")
        # Tr·∫£ v·ªÅ DataFrame r·ªóng ƒë·ªÉ server kh√¥ng b·ªã crash h·∫≥n
        return pd.DataFrame()

_df_cache = None

def get_df_safe() -> pd.DataFrame: # Th√™m -> pd.DataFrame ƒë·ªÉ IDE hi·ªÉu v√† h·∫øt b√°o v√†ng
    global _df_cache
    if _df_cache is None: 
        _df_cache = load_and_clean_data()
    return _df_cache

# =======================================================
# 2. MODELS & VECTORSTORE
# =======================================================
# D√πng Flash ƒë·ªÉ nhanh, temperature th·∫•p ƒë·ªÉ ch√≠nh x√°c
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", 
    temperature=0, # Gi·ªØ 0 ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c theo t√†i li·ªáu
    safety_settings={
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    }
)
print("‚è≥ ƒêang t·∫£i m√¥ h√¨nh Embeddings...")
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=os.environ.get("GOOGLE_API_KEY"))

def load_vectorstore():
    index_path = "faiss_index"
    if os.path.exists(index_path):
        try:
            return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            print(f"L·ªói load FAISS: {e}")
            return None
    return None

vectorstore = load_vectorstore()
if vectorstore:
    # TƒÉng t·ªëc b·∫±ng c√°ch l·ªçc b·ªõt r√°c ngay t·ª´ ƒë·∫ßu (score_threshold)
    # S·ª≠a trong main.py
    retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 10, 
        # "score_threshold": 0.3  <-- X√ìA HO·∫∂C COMMENT D√íNG N√ÄY ƒêI
    }
)
else:
    print("‚ö†Ô∏è Kh√¥ng c√≥ Vectorstore!")

# =======================================================
# 3. CORE LOGIC (T·ªêI ∆ØU H√ìA: ONE-SHOT BRAIN)
# =======================================================

class AppState(TypedDict):
    question: str
    chat_history: list[str]
    intent_data: dict # Ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch g·ªôp (Safety + Route + Keyword)
    context: str | None
    answer: str | None

# --- MESSAGES ---
EMPATHETIC_SAFETY_MESSAGE = """T√¥i kh√¥ng th·ªÉ cung c·∫•p th√¥ng tin v·ªÅ c√°ch s·ª≠ d·ª•ng thu·ªëc ƒë·ªÉ g√¢y h·∫°i cho b·∫£n th√¢n. C√°c lo·∫°i thu·ªëc ch·ªâ an to√†n khi ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë√∫ng li·ªÅu l∆∞·ª£ng theo ch·ªâ d·∫´n c·ªßa b√°c sƒ© ho·∫∑c d∆∞·ª£c sƒ©. Vi·ªác s·ª≠ d·ª•ng qu√° li·ªÅu c√≥ th·ªÉ g√¢y nguy hi·ªÉm nghi√™m tr·ªçng ƒë·∫øn s·ª©c kh·ªèe v√† t√≠nh m·∫°ng.

N·∫øu b·∫°n ƒëang g·∫∑p kh√≥ khƒÉn ho·∫∑c c√≥ √Ω ƒë·ªãnh t·ª± t·ª≠, xin h√£y t√¨m ki·∫øm s·ª± gi√∫p ƒë·ª° ngay l·∫≠p t·ª©c. C√≥ r·∫•t nhi·ªÅu ngu·ªìn h·ªó tr·ª£ s·∫µn s√†ng l·∫Øng nghe v√† gi√∫p ƒë·ª° b·∫°n. B·∫°n c√≥ th·ªÉ li√™n h·ªá v·ªõi c√°c ƒë∆∞·ªùng d√¢y n√≥ng h·ªó tr·ª£ t√¢m l√Ω ho·∫∑c n√≥i chuy·ªán v·ªõi ng∆∞·ªùi th√¢n, b·∫°n b√®, ho·∫∑c chuy√™n gia y t·∫ø.

M·ªôt s·ªë ƒë∆∞·ªùng d√¢y n√≥ng h·ªó tr·ª£ t√¢m l√Ω t·∫°i Vi·ªát Nam m√† b·∫°n c√≥ th·ªÉ li√™n h·ªá:
* T·ªïng ƒë√†i qu·ªëc gia b·∫£o v·ªá tr·∫ª em 111
* T·ªïng ƒë√†i t∆∞ v·∫•n s·ª©c kh·ªèe t√¢m th·∫ßn 1900 561203
* Ho·∫∑c t√¨m ki·∫øm s·ª± h·ªó tr·ª£ t·ª´ c√°c b·ªánh vi·ªán, ph√≤ng kh√°m chuy√™n khoa t√¢m th·∫ßn g·∫ßn nh·∫•t.

H√£y nh·ªõ r·∫±ng b·∫°n kh√¥ng ƒë∆°n ƒë·ªôc v√† c√≥ s·ª± gi√∫p ƒë·ª° d√†nh cho b·∫°n."""

# --- NODE 1: "THE BRAIN" (G·ªòP SAFETY + ROUTER + EXPANSION) ---
# Prompt n√†y ch·ª©a ƒê·∫¶Y ƒê·ª¶ c√°c √Ω t·ª´ code c≈© c·ªßa b·∫°n, nh∆∞ng g·ªôp l·∫°i ƒë·ªÉ ch·∫°y 1 l·∫ßn.
brain_prompt_template = """
B·∫°n l√† b·ªô n√£o trung t√¢m c·ªßa h·ªá th·ªëng AI y t·∫ø Long Ch√¢u. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c√¢u h·ªèi v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON.

H√ÉY TH·ª∞C HI·ªÜN 3 B∆Ø·ªöC PH√ÇN T√çCH SAU:

B∆Ø·ªöC 1: KI·ªÇM DUY·ªÜT AN TO√ÄN (SAFETY CHECK)
Ph√¢n t√≠ch xem c√¢u h·ªèi c√≥ ch·ª©a √Ω ƒë·ªãnh nguy hi·ªÉm kh√¥ng.
- ƒê·∫∂T "is_unsafe": true KHI V√Ä CH·ªà KHI:
   + Ng∆∞·ªùi d√πng c√≥ √Ω ƒë·ªãnh T·ª∞ T·ª¨, T·ª∞ H·∫†I r√µ r√†ng (mu·ªën ch·∫øt, t√¨m c√°ch ch·∫øt, h·ªèi li·ªÅu g√¢y t·ª≠ vong).
   + H·ªèi c√°ch ƒê·∫¶U ƒê·ªòC ng∆∞·ªùi kh√°c.
   + H·ªèi mua thu·ªëc c·∫•m/ma t√∫y.

- ƒê·∫∂T "is_unsafe": false (AN TO√ÄN) KHI:
   + Ng∆∞·ªùi d√πng h·ªèi v·ªÅ "l·ª° u·ªëng qu√° li·ªÅu", "u·ªëng nh·∫ßm", "qu√™n li·ªÅu".
   + Ng∆∞·ªùi d√πng lo l·∫Øng v·ªÅ t√°c d·ª•ng ph·ª• khi u·ªëng nhi·ªÅu (V√≠ d·ª•: "U·ªëng 2 vi√™n c√≥ sao kh√¥ng?").
   -> TR∆Ø·ªúNG H·ª¢P N√ÄY C·∫¶N TR·∫¢ V·ªÄ false ƒê·ªÇ H·ªÜ TH·ªêNG T√åM KI·∫æM TH√îNG TIN T∆Ø V·∫§N C√ÅCH X·ª¨ L√ù.

B∆Ø·ªöC 2: ƒê·ªäNH TUY·∫æN (ROUTING)
X√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi ƒë·ªÉ ch·ªçn ngu·ªìn d·ªØ li·ªáu:
- ∆Øu ti√™n ch·ªçn "structured_analysis" (Pandas) n·∫øu c√¢u h·ªèi ch·ª©a c√°c TI√äU CH√ç L·ªåC C·ª§ TH·ªÇ:
   + H·ªèi v·ªÅ GI√Å C·∫¢ (r·∫ª nh·∫•t, ƒë·∫Øt nh·∫•t, bao nhi√™u ti·ªÅn).
   + H·ªèi v·ªÅ D·∫†NG B√ÄO CH·∫æ (d·∫°ng g√≥i, d·∫°ng vi√™n, siro, thu·ªëc b√¥i, thu·ªëc n∆∞·ªõc, h·ªón d·ªãch).
   + H·ªèi v·ªÅ XU·∫§T X·ª® (c·ªßa M·ªπ, c·ªßa Ph√°p, n∆∞·ªõc n√†o s·∫£n xu·∫•t).
   + H·ªèi v·ªÅ QUY C√ÅCH (h·ªôp bao nhi√™u vi√™n).
- Ch·ªâ ch·ªçn "vector_search" khi h·ªèi thu·∫ßn t√∫y v·ªÅ ki·∫øn th·ª©c: C√¥ng d·ª•ng l√† g√¨? C√°ch d√πng th·∫ø n√†o? B·ªánh n√†y u·ªëng thu·ªëc g√¨ (kh√¥ng y√™u c·∫ßu d·∫°ng c·ª• th·ªÉ)?
-> G√°n gi√° tr·ªã v√†o tr∆∞·ªùng "route".

B∆Ø·ªöC 3: M·ªû R·ªòNG C√ÇU H·ªéI (QUERY EXPANSION)
Chuy·ªÉn ƒë·ªïi c√¢u h·ªèi th√†nh t·ª´ kh√≥a t√¨m ki·∫øm chuy√™n s√¢u:
1. Lu√¥n th√™m t·ª´ kh√≥a "Thu·ªëc", "ƒêi·ªÅu tr·ªã", "D∆∞·ª£c ph·∫©m".
2. N·∫øu m√¥ t·∫£ tri·ªáu ch·ª©ng, th√™m t√™n c√°c HO·∫†T CH·∫§T (Active Ingredients) ph·ªï bi·∫øn.
3. ƒê·ªçc L·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ gi·∫£i quy·∫øt ƒë·∫°i t·ª´ nh√¢n x∆∞ng (n√≥, thu·ªëc n√†y) n·∫øu c·∫ßn.
-> G√°n k·∫øt qu·∫£ v√†o tr∆∞·ªùng "keywords".

INPUT DATA:
History: {chat_history}
Question: {question}

OUTPUT JSON FORMAT (Kh√¥ng ƒë∆∞·ª£c ph√©p tr·∫£ v·ªÅ Markdown, ch·ªâ JSON thu·∫ßn):
{{
    "is_unsafe": boolean,
    "route": "vector_search" | "structured_analysis",
    "keywords": "string"
}}
"""
brain_chain = PromptTemplate.from_template(brain_prompt_template) | llm | JsonOutputParser()

async def brain_node(state: AppState):
    print("--- üß† THE BRAIN IS THINKING (One-Shot) ---")
    question = state["question"]
    # Ch·ªâ l·∫•y 4 c√¢u g·∫ßn nh·∫•t ƒë·ªÉ prompt kh√¥ng qu√° d√†i nh∆∞ng v·∫´n ƒë·ªß context
    history = "\n".join(state.get("chat_history", [])[-4:]) 
    
    # Keyword check nhanh (L·ªõp th·ªß c√¥ng)
    danger_keywords = ["t·ª± t·ª≠", "mu·ªën ch·∫øt", "t·ª± s√°t", "li·ªÅu ch·∫øt", "t·ª± v·∫´n", "quy√™n sinh", "ƒë·∫ßu ƒë·ªôc", "c·∫Øt c·ªï", "u·ªëng thu·ªëc ƒë·ªôc"]
    if any(k in question.lower() for k in danger_keywords):
        print("!!! SAFETY TRIGGERED (KEYWORD) !!!")
        return {"intent_data": {"is_unsafe": True, "route": "none", "keywords": ""}}

    # AI Check (L·ªõp th√¥ng minh)
    try:
        result = await brain_chain.ainvoke({"question": question, "chat_history": history})
        print(f"Brain Analysis: {result}")
        return {"intent_data": result}
    except Exception as e:
        print(f"Brain Error: {e}. Fallback to vector search.")
        # Fallback an to√†n n·∫øu l·ªói JSON
        return {"intent_data": {"is_unsafe": False, "route": "vector_search", "keywords": question}}

# --- NODE 2: RETRIEVE ---
# --- S·ª¨A L·∫†I HAM RETRIEVE_NODE ---
async def retrieve_node(state: AppState):
    print("--- üîç RETRIEVE ---")
    query = state["intent_data"].get("keywords", state["question"])
    print(f"Searching: {query}")
    
    docs = await retriever.ainvoke(query)
    
    # --- DEBUG LOG QUAN TR·ªåNG ---
    if not docs:
        print("‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o (Docs r·ªóng)!")
        return {"context": ""}
        
    print(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(docs)} t√†i li·ªáu.")
    context = "\n\n".join([doc.page_content for doc in docs])
    print(f"   -> T·ªïng ƒë·ªô d√†i Context: {len(context)} k√Ω t·ª±")
    # ----------------------------

    return {"context": context}

# --- NODE 3: PANDAS (Prompt ƒê·∫ßy ƒê·ªß C≈©) ---
pandas_prompt_template = """
B·∫°n c√≥ m·ªôt pandas DataFrame t√™n l√† `df` ch·ª©a d·ªØ li·ªáu thu·ªëc.
C√°c c·ªôt quan tr·ªçng c·∫ßn d√πng: 
- 'T√™n thu·ªëc'
- 'price_int' (Gi√° b√°n d·∫°ng s·ªë nguy√™n. 0 nghƒ©a l√† "Li√™n h·ªá nh√† thu·ªëc").
- 'Gi√° b√°n' (Gi√° d·∫°ng chu·ªói hi·ªÉn th·ªã, v√≠ d·ª•: "570.000ƒë").
- 'Danh m·ª•c' (V√≠ d·ª•: "D·∫ßu c√°, Omega 3, DHA", "Thu·ªëc gi·∫£m ƒëau").
- 'Xu·∫•t x·ª© th∆∞∆°ng hi·ªáu' (V√≠ d·ª•: "Hoa K·ª≥", "Ph√°p").
- 'N∆∞·ªõc s·∫£n xu·∫•t' (V√≠ d·ª•: "Ba Lan", "Vi·ªát Nam").
- 'D·∫°ng b√†o ch·∫ø' (Vi√™n n√©n, Siro, Vi√™n nang m·ªÅm...).
- 'Quy c√°ch' (V√≠ d·ª•: "H·ªôp 6 V·ªâ x 20 Vi√™n").

Nhi·ªám v·ª•: Vi·∫øt M·ªòT d√≤ng code Python ƒë·ªÉ l·ªçc d·ªØ li·ªáu v√† tr·∫£ l·ªùi c√¢u h·ªèi.
K·∫øt qu·∫£ ph·∫£i ƒë∆∞·ª£c g√°n v√†o bi·∫øn `result`.

QUY T·∫ÆC LOGIC QUAN TR·ªåNG:
1. NG·ªÆ C·∫¢NH (CONTEXT):
   - ƒê·ªçc k·ªπ "L·ªãch s·ª≠ tr√≤ chuy·ªán". N·∫øu c√¢u h·ªèi hi·ªán t·∫°i thi·∫øu ch·ªß ng·ªØ (v√≠ d·ª•: "t√¨m d·∫°ng vi√™n", "lo·∫°i n√†o r·∫ª h∆°n"), H√ÉY L·∫§Y T√äN B·ªÜNH HO·∫∂C T√äN THU·ªêC T·ª™ L·ªäCH S·ª¨ √ÅP V√ÄO.
   - V√≠ d·ª•: History n√≥i v·ªÅ "ƒëau d·∫° d√†y". User h·ªèi "t√¨m d·∫°ng vi√™n". -> Code ph·∫£i t√¨m thu·ªëc "ƒëau d·∫° d√†y" V√Ä "d·∫°ng vi√™n".

2. L·ªçc theo "D·∫°ng b√†o ch·∫ø":
   - N·∫øu h·ªèi "D·∫°ng g√≥i": T√¨m ch·ª©a 'G√≥i' OR 'B·ªôt' OR 'H·ªón d·ªãch' OR 'Gel' OR 'Dung d·ªãch'.
   - N·∫øu h·ªèi "D·∫°ng vi√™n": T√¨m ch·ª©a 'Vi√™n' OR 'Nang'.
   
3. L·ªçc theo "B·ªánh/C√¥ng d·ª•ng": T√¨m trong C·∫¢ 3 C·ªòT: `Danh m·ª•c` OR `T√™n thu·ªëc` OR `C√¥ng d·ª•ng`.
4. Lu√¥n th√™m ƒëi·ªÅu ki·ªán `df['price_int'] > 0`.
5. K·∫æT QU·∫¢: Lu√¥n hi·ªÉn th·ªã c·ªôt `T√™n thu·ªëc`, `Gi√° b√°n`, `D·∫°ng b√†o ch·∫ø`.

V√≠ d·ª• 1:
Question: T√¨m 3 lo·∫°i thu·ªëc Omega 3 r·∫ª nh·∫•t.
Python: result = df[(df['Danh m·ª•c'].str.contains('Omega 3', case=False, na=False)) & (df['price_int'] > 0)].nsmallest(3, 'price_int')[['T√™n thu·ªëc', 'Gi√° b√°n', 'Quy c√°ch', 'Xu·∫•t x·ª© th∆∞∆°ng hi·ªáu']].to_string()

V√≠ d·ª• 2:
Question: C√≥ bao nhi√™u lo·∫°i thu·ªëc c·ªßa M·ªπ?
Python: result = f"C√≥ {{len(df[(df['N∆∞·ªõc s·∫£n xu·∫•t'].str.contains('M·ªπ|Hoa K·ª≥|USA', case=False, na=False)) | (df['Xu·∫•t x·ª© th∆∞∆°ng hi·ªáu'].str.contains('M·ªπ|Hoa K·ª≥|USA', case=False, na=False))])}} thu·ªëc c√≥ xu·∫•t x·ª© ho·∫∑c th∆∞∆°ng hi·ªáu M·ªπ."

V√≠ d·ª• 3:
Question: Li·ªát k√™ c√°c thu·ªëc d·∫°ng Siro gi√° d∆∞·ªõi 50000.
Python: result = df[(df['D·∫°ng b√†o ch·∫ø'].str.contains('Siro', case=False, na=False)) & (df['price_int'] > 0) & (df['price_int'] < 50000)][['T√™n thu·ªëc', 'Gi√° b√°n', 'Quy c√°ch']].to_string()

QUY T·∫ÆC LOGIC QUAN TR·ªåNG:
1. NG·ªÆ C·∫¢NH (CONTEXT):
   - ƒê·ªçc k·ªπ "L·ªãch s·ª≠ tr√≤ chuy·ªán". N·∫øu c√¢u h·ªèi hi·ªán t·∫°i thi·∫øu ch·ªß ng·ªØ (v√≠ d·ª•: "t√¨m d·∫°ng vi√™n", "lo·∫°i n√†o r·∫ª h∆°n"), H√ÉY L·∫§Y T√äN B·ªÜNH HO·∫∂C T√äN THU·ªêC T·ª™ L·ªäCH S·ª¨ √ÅP V√ÄO.
   - V√≠ d·ª•: History n√≥i v·ªÅ "ƒëau d·∫° d√†y". User h·ªèi "t√¨m d·∫°ng vi√™n". -> Code ph·∫£i t√¨m thu·ªëc "ƒëau d·∫° d√†y" V√Ä "d·∫°ng vi√™n".

2. L·ªçc theo "D·∫°ng b√†o ch·∫ø":
   - N·∫øu h·ªèi "D·∫°ng g√≥i": T√¨m ch·ª©a 'G√≥i' OR 'B·ªôt' OR 'H·ªón d·ªãch' OR 'Gel' OR 'Dung d·ªãch'.
   - N·∫øu h·ªèi "D·∫°ng vi√™n": T√¨m ch·ª©a 'Vi√™n' OR 'Nang'.
   
3. L·ªçc theo "B·ªánh/C√¥ng d·ª•ng": T√¨m trong C·∫¢ 3 C·ªòT: `Danh m·ª•c` OR `T√™n thu·ªëc` OR `C√¥ng d·ª•ng`.
4. Lu√¥n th√™m ƒëi·ªÅu ki·ªán `df['price_int'] > 0`.
5. K·∫æT QU·∫¢: Lu√¥n hi·ªÉn th·ªã c·ªôt `T√™n thu·ªëc`, `Gi√° b√°n`, `D·∫°ng b√†o ch·∫ø`.

L·ªãch s·ª≠ tr√≤ chuy·ªán: {chat_history}
Question: {question}
Python:
"""
pandas_chain = PromptTemplate.from_template(pandas_prompt_template) | llm | StrOutputParser()

async def structured_analysis_node(state: AppState):
    print("--- üêº PANDAS ANALYSIS ---")
    question = state["question"]
    # 1. L·∫•y l·ªãch s·ª≠ chat ƒë·ªÉ Pandas hi·ªÉu ng·ªØ c·∫£nh
    history = "\n".join(state.get("chat_history", [])[-4:]) # L·∫•y 4 c√¢u g·∫ßn nh·∫•t
    df = get_df_safe()
    
    # 2. Truy·ªÅn th√™m chat_history v√†o invoke
    code = await pandas_chain.ainvoke({
        "question": question, 
        "chat_history": history # <--- QUAN TR·ªåNG: Truy·ªÅn history v√†o ƒë√¢y
    })
    
    clean_code = code.replace("```python", "").replace("```", "").strip()
    print(f"Generated Code: {clean_code}") # In ra ƒë·ªÉ debug xem n√≥ c√≥ l·ªçc ƒë√∫ng 'd·∫° d√†y' kh√¥ng

    local_vars = {"df": df, "result": None}
    try:
        exec(clean_code, {}, local_vars)
        result = local_vars["result"]
        final = result.to_string() if hasattr(result, 'to_string') else str(result)
        final_answer = f"S·ªë li·ªáu t√¨m ƒë∆∞·ª£c:\n{final}"
    except Exception as e:
        final_answer = f"L·ªói t√≠nh to√°n: {e}"
        
    return {"answer": final_answer}

# --- NODE 4: GENERATE (Prompt ƒê·∫ßy ƒê·ªß C≈©) ---
# Prompt n√†y gi·ªØ nguy√™n y h·ªát b·∫£n g·ªëc c·ªßa b·∫°n
generate_prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω t∆∞ v·∫•n thu·ªëc th√¥ng minh c·ªßa Long Ch√¢u.

NGUY√äN T·∫ÆC AN TO√ÄN TUY·ªÜT ƒê·ªêI (SAFETY GUARDRAILS):
1. T·ª∞ T·ª¨ & L√ÄM H·∫†I B·∫¢N TH√ÇN: N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ li·ªÅu l∆∞·ª£ng g√¢y ch·∫øt ng∆∞·ªùi, c√°ch t·ª± t·ª≠... -> T·ª™ CH·ªêI TR·∫¢ L·ªúI.
2. QU√Å LI·ªÄU/U·ªêNG NH·∫¶M: C·∫£nh b√°o ƒëi kh√°m b√°c sƒ©, sau ƒë√≥ cung c·∫•p th√¥ng tin tham kh·∫£o t·ª´ Context.
3. KH√îNG THAY TH·∫æ B√ÅC Sƒ®: V·ªõi c√°c tri·ªáu ch·ª©ng nghi√™m tr·ªçng, khuy√™n ƒëi kh√°m ngay.
4. KH√îNG B·ªäA ƒê·∫∂T: Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n Context v√† L·ªãch s·ª≠.

L·ªãch s·ª≠ h·ªôi tho·∫°i:
{chat_history}

Context:
{context}

Question: {question}
Answer:
"""
rag_generation_chain = PromptTemplate.from_template(generate_prompt_template) | llm | StrOutputParser()

async def generate_node(state: AppState):
    print("--- ‚úçÔ∏è GENERATE ---")
    question = state["question"]
    context = state.get("context", "")
    
    # N·∫øu kh√¥ng c√≥ context th√¨ b√°o ngay
    if not context:
        print("‚ö†Ô∏è Context r·ªóng, b·ªè qua b∆∞·ªõc g·ªçi LLM.")
        answer = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ lo·∫°i thu·ªëc n√†y trong c∆° s·ªü d·ªØ li·ªáu."
    else:
        history = "\n".join(state.get("chat_history", []))
        try:
            answer = await rag_generation_chain.ainvoke({
                "question": question, 
                "context": context, 
                "chat_history": history
            })
            # DEBUG: In c√¢u tr·∫£ l·ªùi ra terminal xem n√≥ c√≥ b·ªã r·ªóng kh√¥ng
            print(f"ü§ñ AI Answer: {answer}") 
        except Exception as e:
            print(f"Error Generate: {e}")
            answer = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi."
    
    new_history = state.get("chat_history", []) + [f"User: {question}", f"AI: {answer}"]
    return {"answer": answer, "chat_history": new_history}

async def update_history_pandas(state: AppState):
    # Node ph·ª• ƒë·ªÉ c·∫≠p nh·∫≠t l·ªãch s·ª≠ cho nh√°nh Pandas (v√¨ Pandas Node return answer tr·ª±c ti·∫øp)
    question = state["question"]
    answer = state["answer"]
    new_history = state.get("chat_history", []) + [f"User: {question}", f"AI: {answer}"]
    return {"chat_history": new_history}

# =======================================================
# 4. X√ÇY D·ª∞NG GRAPH
# =======================================================
def build_rag_agent():
    workflow = StateGraph(AppState)

    # Add Nodes
    workflow.add_node("brain", brain_node)
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("structured_analysis", structured_analysis_node)
    workflow.add_node("update_history_pandas", update_history_pandas) # Node ph·ª• ƒë·ªÉ l∆∞u history

    # Entry Point
    workflow.set_entry_point("brain")

    # Routing Logic
    def route_decision(state):
        intent = state["intent_data"]
        
        # 1. N·∫øu kh√¥ng an to√†n -> End ngay (tr·∫£ l·ªùi ·ªü API handler)
        if intent.get("is_unsafe"):
            print("--- ROUTING: UNSAFE -> END ---")
            return "unsafe"
            
        # 2. ƒê·ªãnh tuy·∫øn b√¨nh th∆∞·ªùng
        route = intent.get("route")
        print(f"--- ROUTING TO: {route} ---")
        if route == "structured_analysis":
            return "structured_analysis"
        else:
            return "vector_search"

    workflow.add_conditional_edges(
        "brain",
        route_decision,
        {
            "structured_analysis": "structured_analysis",
            "vector_search": "retrieve",
            "unsafe": END
        }
    )

    # Edges
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)
    
    # Nh√°nh Pandas: T√≠nh to√°n -> L∆∞u history -> End
    workflow.add_edge("structured_analysis", "update_history_pandas")
    workflow.add_edge("update_history_pandas", END)

    return workflow.compile(checkpointer=MemorySaver())

rag_agent = build_rag_agent()
print("üöÄ TURBO BACKEND (FULL PROMPTS) READY!")

# =======================================================
# 5. API ENDPOINT
# =======================================================
class ChatRequest(BaseModel):
    question: str
    thread_id: str = "default_user"

@app.post("/chat")
async def chat_handler(request: ChatRequest):
    print(f"--> User: {request.question}")
    config = {"configurable": {"thread_id": request.thread_id}}
    
    result = await rag_agent.ainvoke({"question": request.question}, config=config)
    
    # Ki·ªÉm tra Safety t·ª´ k·∫øt qu·∫£ Brain
    intent = result.get("intent_data", {})
    if intent.get("is_unsafe"):
        final_answer = EMPATHETIC_SAFETY_MESSAGE
    else:
        final_answer = result.get("answer", "L·ªói: Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.")
    
    return {"answer": final_answer}

# =======================================================
# 6. AUDIO ENDPOINTS 
# =======================================================

# --- A. Text-to-Speech (TTS) - D√πng Edge-TTS (Mi·ªÖn ph√≠, Gi·ªçng hay) ---
@app.post("/tts")
async def text_to_speech(request: ChatRequest):
    """
    Nh·∫≠n text -> Tr·∫£ v·ªÅ file √¢m thanh MP3 (Streaming)
    """
    text = request.question # L·∫•y ƒëo·∫°n vƒÉn b·∫£n c·∫ßn ƒë·ªçc
    voice = "vi-VN-HoaiMyNeural" # Gi·ªçng n·ªØ mi·ªÅn B·∫Øc c·ª±c chu·∫©n
    # voice = "vi-VN-NamMinhNeural" # Gi·ªçng nam (n·∫øu th√≠ch)

    # T·∫°o giao ti·∫øp v·ªõi Edge TTS
    communicate = edge_tts.Communicate(text, voice)
    
    # T·∫°o b·ªô nh·ªõ ƒë·ªám ƒë·ªÉ ch·ª©a √¢m thanh (kh√¥ng c·∫ßn l∆∞u file r√°c v√†o ·ªï c·ª©ng)
    audio_stream = io.BytesIO()
    
    # Ghi d·ªØ li·ªáu v√†o stream
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            audio_stream.write(chunk["data"])
    
    audio_stream.seek(0) # Tua l·∫°i ƒë·∫ßu bƒÉng
    
    # Tr·∫£ v·ªÅ d·∫°ng stream ƒë·ªÉ Frontend ph√°t ƒë∆∞·ª£c ngay
    return StreamingResponse(audio_stream, media_type="audio/mpeg")

# --- B. Speech-to-Text (STT) - D√πng Gemini qua REST API (Kh√¥ng xung ƒë·ªôt) ---
@app.post("/stt")
async def speech_to_text(file: UploadFile = File(...)):
    """
    Nh·∫≠n file √¢m thanh -> G·ª≠i tr·ª±c ti·∫øp qua HTTP Request t·ªõi Gemini
    """
    # 1. ƒê·ªçc d·ªØ li·ªáu file
    file_bytes = await file.read()
    
    # 2. M√£ h√≥a sang Base64 (ƒê·ªÉ g·ª≠i qua m·∫°ng)
    base64_audio = base64.b64encode(file_bytes).decode('utf-8')
    
    # 3. X√°c ƒë·ªãnh Mime Type (Mp3, Wav, Webm...)
    mime_type = "audio/mp3" # M·∫∑c ƒë·ªãnh
    if file.filename.endswith(".wav"): mime_type = "audio/wav"
    elif file.filename.endswith(".webm"): mime_type = "audio/webm"
    
    # 4. C·∫•u h√¨nh Key & URL
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        return {"text": "L·ªói: Ch∆∞a c·∫•u h√¨nh GOOGLE_API_KEY"}
        
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"
    
    # 5. T·∫°o Payload (G√≥i tin g·ª≠i ƒëi)
    payload = {
        "contents": [{
            "parts": [
                {"text": "H√£y nghe ƒëo·∫°n √¢m thanh n√†y v√† ch√©p l·∫°i nguy√™n vƒÉn n·ªôi dung b·∫±ng ti·∫øng Vi·ªát. Ch·ªâ tr·∫£ v·ªÅ n·ªôi dung vƒÉn b·∫£n, kh√¥ng th√™m l·ªùi d·∫´n."},
                {
                    "inline_data": {
                        "mime_type": mime_type,
                        "data": base64_audio
                    }
                }
            ]
        }]
    }

    try:
        # 6. G·ª≠i Request
        print("üì§ ƒêang g·ª≠i Inline Audio t·ªõi Gemini (REST API)...")
        response = requests.post(url, json=payload, headers={"Content-Type": "application/json"})
        
        # 7. X·ª≠ l√Ω k·∫øt qu·∫£
        if response.status_code == 200:
            result_json = response.json()
            try:
                text_result = result_json["candidates"][0]["content"]["parts"][0]["text"].strip()
                print(f"üéôÔ∏è Gemini nghe ƒë∆∞·ª£c: {text_result}")
                return {"text": text_result}
            except KeyError:
                print(f"‚ùå Gemini kh√¥ng tr·∫£ v·ªÅ text. Response: {result_json}")
                return {"text": ""}
        else:
            print(f"‚ùå L·ªói API ({response.status_code}): {response.text}")
            return {"text": ""}

    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi STT: {e}")
        return {"text": ""}
    
# =======================================================
# 7. UNIFIED ENDPOINT (ALL-IN-ONE)
# =======================================================
@app.post("/chat-voice-flow")
async def chat_voice_flow(
    file: UploadFile = File(...), 
    thread_id: str = "default_user"
):
    """
    Quy tr√¨nh Full: Nh·∫≠n Audio -> STT -> Agent x·ª≠ l√Ω -> TTS -> Tr·∫£ v·ªÅ JSON (Text + Audio Base64)
    """
    print(f"üé§ Nh·∫≠n y√™u c·∫ßu Voice Chat t·ª´ user: {thread_id}")

    # --- B∆Ø·ªöC 1: STT (Speech to Text) ---
    # T√°i s·ª≠ d·ª•ng logic g·ªçi Gemini API
    file_bytes = await file.read()
    base64_audio = base64.b64encode(file_bytes).decode('utf-8')
    
    # X√°c ƒë·ªãnh lo·∫°i file
    mime_type = "audio/mp3"
    if file.filename.endswith(".wav"): mime_type = "audio/wav"
    elif file.filename.endswith(".webm"): mime_type = "audio/webm"

    api_key = os.environ.get("GOOGLE_API_KEY")
    stt_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"
    
    stt_payload = {
        "contents": [{
            "parts": [
                {"text": "H√£y nghe ƒëo·∫°n √¢m thanh n√†y v√† ch√©p l·∫°i nguy√™n vƒÉn n·ªôi dung b·∫±ng ti·∫øng Vi·ªát. Ch·ªâ tr·∫£ v·ªÅ n·ªôi dung vƒÉn b·∫£n, kh√¥ng th√™m l·ªùi d·∫´n."},
                {"inline_data": {"mime_type": mime_type, "data": base64_audio}}
            ]
        }]
    }

    user_text = ""
    try:
        resp = requests.post(stt_url, json=stt_payload, headers={"Content-Type": "application/json"})
        if resp.status_code == 200:
            user_text = resp.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
            print(f"   -> Nghe ƒë∆∞·ª£c: {user_text}")
        else:
            print(f"   -> L·ªói STT: {resp.text}")
            return {"error": "Kh√¥ng nghe r√µ gi·ªçng n√≥i"}
    except Exception as e:
        return {"error": f"L·ªói k·∫øt n·ªëi STT: {str(e)}"}

    if not user_text:
        return {"answer": "T√¥i kh√¥ng nghe th·∫•y g√¨ c·∫£.", "audio_base64": None}

    # --- B∆Ø·ªöC 2: AGENT THINKING (LangGraph) ---
    print(f"   -> Agent ƒëang suy nghƒ©...")
    config = {"configurable": {"thread_id": thread_id}}
    result = await rag_agent.ainvoke({"question": user_text}, config=config)
    
    # X·ª≠ l√Ω Safety
    intent = result.get("intent_data", {})
    if intent.get("is_unsafe"):
        bot_answer = EMPATHETIC_SAFETY_MESSAGE
    else:
        bot_answer = result.get("answer", "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω th√¥ng tin.")
    
    print(f"   -> Bot tr·∫£ l·ªùi: {bot_answer}")

    # --- B∆Ø·ªöC 3: TTS (Text to Speech) ---
    print(f"   -> ƒêang chuy·ªÉn vƒÉn b·∫£n sang gi·ªçng n√≥i...")
    communicate = edge_tts.Communicate(bot_answer, "vi-VN-HoaiMyNeural")
    
    # Ghi audio v√†o b·ªô nh·ªõ ƒë·ªám
    audio_stream = io.BytesIO()
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            audio_stream.write(chunk["data"])
    
    # Chuy·ªÉn audio th√†nh Base64 ƒë·ªÉ g·ª≠i k√®m JSON
    audio_base64 = base64.b64encode(audio_stream.getvalue()).decode('utf-8')

    # --- K·∫æT QU·∫¢ TR·∫¢ V·ªÄ ---
    return {
        "user_text": user_text,   # ƒê·ªÉ hi·ªán l√™n m√†n h√¨nh chat ph√≠a user
        "bot_answer": bot_answer, # ƒê·ªÉ hi·ªán c√¢u tr·∫£ l·ªùi ch·ªØ
        "audio_base64": audio_base64 # ƒê·ªÉ Frontend ph√°t √¢m thanh
    }