# --- 1. C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN ---
!pip install -q langchain langchain-community faiss-cpu langchain-google-genai

import json
import os
import time
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings 
from kaggle_secrets import UserSecretsClient 

print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh vector h√≥a d·ªØ li·ªáu (FULL DETAIL VERSION)...")

# --- 2. C·∫§U H√åNH ---
# Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file input c·ªßa b·∫°n
JSON_FILE_PATH = "/kaggle/input/data-longchau/longchau_selected.json" 
VECTOR_STORE_PATH = "/kaggle/working/faiss_index"

# --- 3. KH·ªûI T·∫†O API & MODEL ---
print("üîë ƒêang l·∫•y API Key...")
try:
    user_secrets = UserSecretsClient()
    api_key = user_secrets.get_secret("GOOGLE_API_KEY")
    os.environ["GOOGLE_API_KEY"] = api_key
except Exception as e:
    print("‚ùå L·ªñI: Ch∆∞a c·∫•u h√¨nh Secret 'GOOGLE_API_KEY'.")
    raise e

print("‚è≥ ƒêang t·∫£i m√¥ h√¨nh Google Embeddings (text-embedding-004)...")
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# --- 4. ƒê·ªåC D·ªÆ LI·ªÜU & T·∫†O CONTENT CHI TI·∫æT ---
print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ file: {JSON_FILE_PATH}")
documents = []

try:
    with open(JSON_FILE_PATH, "r", encoding="utf-8") as f:
        data_array = json.load(f) 
        
    print(f"   -> T√¨m th·∫•y {len(data_array)} d√≤ng d·ªØ li·ªáu th√¥.")
    
    for product in data_array:
        try:
            # 1. L·∫•y th√¥ng tin c∆° b·∫£n
            name = product.get("T√™n thu·ªëc") or product.get("product_name") or ""
            if not name: continue 

            # 2. L·∫•y th√¥ng tin chi ti·∫øt (∆Øu ti√™n ti·∫øng Vi·ªát, fallback sang ti·∫øng Anh)
            # H√†m get an to√†n: l·∫•y value, n·∫øu k c√≥ tr·∫£ v·ªÅ chu·ªói r·ªóng
            def get_safe(key_vi, key_en):
                val = product.get(key_vi) or product.get(key_en) or ""
                return str(val).strip()

            danh_muc = get_safe("Danh m·ª•c", "category")
            thanh_phan = get_safe("Th√†nh ph·∫ßn", "active_ingredient").replace("Th√¥ng tin th√†nh ph·∫ßn H√†m l∆∞·ª£ng", "")
            cong_dung = get_safe("C√¥ng d·ª•ng", "indications")
            lieu_dung = get_safe("Li·ªÅu d√πng", "usage_instructions")
            
            # --- QUAN TR·ªåNG: C√ÅC TR∆Ø·ªúNG "S√ÇU" M√Ä B·∫†N C·∫¶N ---
            tac_dung_phu = get_safe("T√°c d·ª•ng ph·ª•", "side_effects")
            luu_y = get_safe("L∆∞u √Ω", "precautions") # Ch·ª©a th√¥ng tin v·ªÅ gan, th·∫≠n
            chong_chi_dinh = get_safe("Ch·ªëng ch·ªâ ƒë·ªãnh", "contraindications") # Ch·ª©a th√¥ng tin v·ªÅ b√† b·∫ßu, tr·∫ª em
            bao_quan = get_safe("B·∫£o qu·∫£n", "preservation")
            
            nha_san_xuat = get_safe("Nh√† s·∫£n xu·∫•t", "manufacturer")
            nuoc_san_xuat = get_safe("N∆∞·ªõc s·∫£n xu·∫•t", "country_of_origin")
            xuat_xu = get_safe("Xu·∫•t x·ª© th∆∞∆°ng hi·ªáu", "brand_origin")
            dang_bao_che = get_safe("D·∫°ng b√†o ch·∫ø", "form")
            quy_cach = get_safe("Quy c√°ch", "packaging")

            # 3. X√¢y d·ª±ng Page Content "Si√™u ƒë·∫ßy ƒë·ªß"
            # AI s·∫Ω ƒë·ªçc ƒëo·∫°n vƒÉn b·∫£n n√†y ƒë·ªÉ tr·∫£ l·ªùi. C√†ng chi ti·∫øt c√†ng t·ªët.
            page_content = f"""
            T√™n s·∫£n ph·∫©m: {name}
            Danh m·ª•c: {danh_muc}
            D·∫°ng b√†o ch·∫ø: {dang_bao_che}
            Quy c√°ch ƒë√≥ng g√≥i: {quy_cach}
            Xu·∫•t x·ª©: Th∆∞∆°ng hi·ªáu {xuat_xu}, S·∫£n xu·∫•t t·∫°i {nuoc_san_xuat} b·ªüi {nha_san_xuat}.

            TH√ÄNH PH·∫¶N:
            {thanh_phan}

            C√îNG D·ª§NG & CH·ªà ƒê·ªäNH:
            {cong_dung}

            C√ÅCH D√ôNG & LI·ªÄU D√ôNG:
            {lieu_dung}

            CH·ªêNG CH·ªà ƒê·ªäNH (Kh√¥ng d√πng cho):
            {chong_chi_dinh}

            L∆ØU √ù & TH·∫¨N TR·ªåNG (C·∫£nh b√°o an to√†n):
            {luu_y}

            T√ÅC D·ª§NG PH·ª§ C√ì TH·ªÇ G·∫∂P:
            {tac_dung_phu}
            
            B·∫¢O QU·∫¢N:
            {bao_quan}
            """.strip()

            # 4. Metadata (D√πng ƒë·ªÉ l·ªçc n·∫øu c·∫ßn, ho·∫∑c hi·ªÉn th·ªã UI)
            metadata = {
                "source": name,
                "price": str(product.get("Gi√° b√°n") or product.get("price_VND") or "0"),
                "origin": xuat_xu
            }
            
            doc = Document(page_content=page_content, metadata=metadata)
            documents.append(doc)
            
        except Exception as e:
            continue 

except FileNotFoundError:
    print(f"‚ùå KH√îNG T√åM TH·∫§Y FILE T·∫†I: {JSON_FILE_PATH}")
    exit()

if len(documents) == 0:
    print("‚ùå C·∫¢NH B√ÅO: Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c s·∫£n ph·∫©m n√†o.")
    exit()

print(f"‚úÖ ƒê√£ chu·∫©n h√≥a FULL DATA cho {len(documents)} s·∫£n ph·∫©m.")

# Chia nh·ªè vƒÉn b·∫£n
# TƒÉng chunk_size l√™n 1500 v√¨ content b√¢y gi·ªù r·∫•t d√†i
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
split_docs = text_splitter.split_documents(documents)
print(f"üì¶ ƒê√£ chia th√†nh {len(split_docs)} chunks.")

# --- 5. T·∫†O VECTOR INDEX ---
print("‚ö° B·∫Øt ƒë·∫ßu t·∫°o Vector Index (Google Version)...")
start_time = time.time()

try:
    vector_db = FAISS.from_documents(split_docs, embeddings)
    vector_db.save_local(VECTOR_STORE_PATH)
    
    end_time = time.time()
    print("-" * 50)
    print(f"üéâ TH√ÄNH C√îNG! FAISS Index (Full Detail) ƒë√£ ƒë∆∞·ª£c t·∫°o.")
    print(f"‚è±Ô∏è Th·ªùi gian: {((end_time - start_time) / 60):.2f} ph√∫t")
    print("-" * 50)
    
    # N√©n file l·∫°i
    !zip -r faiss_index.zip {VECTOR_STORE_PATH}
    print("‚úÖ ƒê√£ n√©n xong: faiss_index.zip. H√£y t·∫£i v·ªÅ ngay!")
    
except Exception as e:
    print(f"‚ùå L·ªói t·∫°o Vector: {e}")